---
id: storage-memory-usage
title: Storage memory usage
sidebar_label: Storage memory usage
---

Estimating Memgraph's storage memory usage is not entirely straightforward
because it depends on a lot of variables, but it is possible to do so quite
accurately. Below is an example that will try to show the basic reasoning.

If you want to **estimate** the storage memory usage, use the following formula:

$\texttt{StorageRAMUsage} = \texttt{NumberOfVertices} \times 260\text{B} + \texttt{NumberOfEdges} \times 180\text{B}$

Let's test this formula on the [Marvel Comic Universe Social Network
dataset](https://memgraph.com/download/datasets/marvel-cinematic-universe/marvel-cinematic-universe.cypherl.gz),
which is also available as a dataset inside Memgraph Lab and contains 21,723
vertices and 682,943 edges. 

According to the formula, storage memory usage should be: 

$
\begin{aligned}
\texttt{StorageRAMUsage} &= 21,723 \times 260\text{B} + 682,943 \times 180\text{B} \\ &= 5,647,980\text{B} + 122,929,740\text{B}\\ &= 128,577,720\text{B} \approx 125\text{MB}
\end{aligned}
$

Now, let's run an empty Memgraph instance on a x86 Ubuntu. It consumes **~75MB**
of RAM due to baseline runtime overhead. Once the dataset is loaded, RAM usage
rises up to **~260MB**. Memory usage primarily consists of storage and query
execution memory usage. After executing `FREE MEMORY` query to force the cleanup
of query execution, the RAM usage drops to **~200MB**. If the baseline runtime
overhead of **75MB** is subtracted from the total memory usage of the dataset,
which is **200MB**, and storage memory usage comes up to **~125MB**, which shows
that the formula is correct.

## The calculation in detail

Let's dive deeper into the memory usage values. Because Memgraph works on the
x86 architecture, calculations are based on the x86 Linux memory usage.

:::tip 
For the latest and most precise memory layout please clone
[Memgraph](https://github.com/memgraph/memgraph) and use, e.g.,
[pahole](https://github.com/PhilArmstrong/pahole-gdb) to discover accurate
information. 
:::

Each `Vertex` and `Edge` object has a pointer to a `Delta` object. The
`Delta` object stores all changes on a certain `Vertex` or `Edge` and that's
why `Vertex` and `Edge` memory usage will be increased by the memory of
the `Delta` objects they are pointing to. If there are few updates, there are
also few `Delta` objects because the latest data is stored in the object.
But, if the database has a lot of concurrent operations, many `Delta` objects
will be created. Of course, the `Delta` objects will be kept in memory as long as
needed, and a bit more, because of the internal GC inefficiencies.

### `Delta` memory layout

Each `Delta` object has a least **104B**.

### `Vertex` memory layout

Each `Vertex` object has at least **112B** + **104B** for the `Delta` object, in
total, a minimum of **216B**.

Each additional label takes **8B**. 

Keep in mind that three labels take as much space as four labels, and five to
seven labels take as much space as eight labels, etc., due to the dynamic
memory allocation.  

### `Edge` memory layout

Each `Edge` object has at least **40B** + **104B** for the `Delta` object, in
total, a minimum of **144B**.

### `SkipList` memory layout

Each object (`Vertex`, `Edge`) is placed inside a data structure
called a `SkipList`. The `SkipList` has an additional overhead in terms of
`SkipListNode` structure and `next_pointers`. Each `SkipListNode` has an
additional **8B** element overhead and another **8B** for each of the `next_pointers`.

It is impossible to know the exact number of **next_pointers** upfront, and
consequently the total size, but it's never more than **double the number of
objects** because the number of pointers is generated by binomial distribution
(take a look at [the source
code](https://github.com/memgraph/memgraph/blob/master/src/utils/skip_list.hpp)
for details).

### Index memory layout

Each `LabelIndex::Entry` object has exactly **16B**.

Depending on the actual value stored, each `LabelPropertyIndex::Entry` has at least **72B**.

Objects of both types are placed into the `SkipList`.

#### Each index object in total

- `SkipListNode<LabelIndex::Entry>` object has **24B**.
- `SkipListNode<LabelPropertyIndex::Entry>` has at least **80B**.
- Each `SkipListNode` has an additional **16B** because of the **next_pointers**.

### Properties

All properties use **1B** for metadata - type, size of property ID and the size
of payload in the case of `NULL` and `BOOLEAN` values, or size of payload size
indicator for other types (how big is the stored value, for example, integers
can be 1B, 2B 4B or 8b depending on their value). 

Then they take up **another byte** for storing property ID, which means each
property takes up at least 2B. After those 2B, some properties (for example,
`STRING` values) store addition metadata. And lastly, all properties store the
value. So the layout of each property is:


$\texttt{propertySize} = \texttt{basicMetadata} + \texttt{propertyID} + [\texttt{additionalMetadata}] + \texttt{value}.$


|Value type       |Size                            |Note
|-----------------|--------------------------------|-----------------------------------------------------------------------------------------------------|
|`NULL`           |1B + 1B                         | The value is written in the first byte of the basic metadata.                                       |
|`BOOL`           |1B + 1B                         | The value is written in the first byte of the basic metadata.
|`INT`            |1B + 1B + 1B, 2B, 4B or 8B      | Basic metadata, property ID and the value depending on the size of the integer.                     |
|`DOUBLE`         |1B + 1B + 8B                    | Basic metadata, property ID and the value                                                           |
|`STRING`         |1B + 1B + 1B + min 1B           | Basic metadata, property ID, additional metadata and lastly the value depending on the size of the string, where 1 ASCII character in the string takes up 1B.|
|`LIST`           |1B + 1B + 1B + min 1B           | Basic metadata, property ID, additional metadata and the total size depends on the number and size of the values in the list.|
|`MAP`            |1B + 1B + 1B + min 1B           | Basic metadata, property ID, additional metadata and the total size depends on the number and size of the values in the map.|
|`TEMPORAL_DATA`  |1B + 1B + 1B + min 1B + min 1B  | Basic metadata, property ID, additional metadata, seconds, microseconds. Value od the seconds and microseconds is at least 1B, but probably 4B in most cases due to the large values they store.|

### Marvel dataset use case

The Marvel dataset consists of `Hero`, `Comic` and `ComicSeries` labels, which
are indexed. There are also three label-property indices - on the `name`
property of `Hero` and `Comic` vertices, and on the `title` property of
`ComicSeries` vertices. The `ComicSeries` vertices also have the `publishYear`
property.

 <img src={require('../data/under-the-hood/marvel-dataset-schema.png').default}/>

There are 6487 `Hero` and 12,661 `Comic` vertices with the property `name`.
That's 19,148 vertices in total. To calculate how much storage those vertices
and properties occupy, we are going to use the following formula:

$\texttt{NumberOfVertices} \times (\texttt{Vertex} + \texttt{properties} + \texttt{SkipListNode} + \texttt{next\_pointers} + \texttt{Delta}).$

Let's assume the name on average has $3\text{B}+10\text{B} = 13\text{B}$ (each
name is on average 10 characters long). One the average values are included, the
calculation is:

$19,148 \times (112\text{B} + 13\text{B} + 16\text{B} + 16\text{B} + 104\text{B}) = 19,148 \times 261\text{B} = 4,997,628\text{B}.$

The remaining 2,584 vertices are the `ComicSeries` vertices with the `title` and
`publishYear` properties. Let's assume that the `title` property is
approximately the same length as the `name` property. The `publishYear` property
is a list of integers. The average length of the `publishYear` list is 2.17,
let's round it up to 3 elements. Since the year is an integer, 2B for each
integer will be more than enough, plus the 2B for the metadata. Therefore, each
list occupies $3 \times 2\text{B} \times 2\text{B} = 12\text{B}$. Using the same
formula as above, but being careful to include both `title` and `publishYear`
properties, the calculation is:

$2584 \times (112\text{B} + 13\text{B} + 12\text{B} + 16\text{B} + 16\text{B} + 104\text{B}) = 2584 \times 273\text{B} = 705,432\text{B}.$

In total, $5,703,060\text{B}$ to store vertices.

The edges don't have any properties on them, so the formula is as follows:

$\texttt{NumberOfEdges} \times (\texttt{Edge} + \texttt{SkipListNode} + \texttt{next\_pointers} + \texttt{Delta}).$

There are 682,943 edges in the Marvel dataset. Hence, we have:

$682,943 \times (40\text{B}+16\text{B}+16\text{B}+104\text{B}) = 682,943 \times 176\text{B} = 120,197,968\text{B}.$

Next, `Hero`, `Comic` and `ComicSeries` labels have label indices. To calculate
how much space they take up, use the following formula:

$\texttt{NumberOfLabelIndices} \times \texttt{NumberOfVertices} \times (\texttt{SkipListNode<LabelIndex::Entry>} + \texttt{next\_pointers}).$

Since there are three label indices, we have the following calculation:

$3 \times 21,723 \times (24\text{B}+16\text{B}) = 65,169 \times 40\text{B} = 2,606,760\text{B}.$

For label-property index, labeled property needs to be taken into account.
Property `name` is indexed on `Hero` and `Comic` vertices, while property
`title` is indexed on `ComicSeries` vertices. We already assumed that the
`title` property is approximately the same length as the `name` property. 

Here is the formula:

$\texttt{NumberOfLabelPropertyIndices} \times \texttt{NumberOfVertices} \times (\texttt{SkipListNode<LabelIndex::Entry>} + \texttt{property} + \texttt{next\_pointers}).$

When the appropriate values are included, the calculation is:

$3 \times 21,723 \times (80\text{B}+13\text{B}+16\text{B})= 65,169 \times 109\text{B} = 7,103,421\text{B}.$

Now let's sum up everything we calculated:

$5,703,060\text{B} + 120,197,968\text{B} + 2,606,760\text{B} + 7,103,421\text{B} = 135,611,209 \text{B} \approx 130\text{MB}.$

Bear in mind the number can vary because objects can have higher overhead due to
the additional data.

## Query execution memory usage

Query execution also uses up RAM. In some cases, intermediate results are
aggregated to return valid query results and the query execution memory can end
up using a large amount of RAM. Keep in mind that query execution memory
monotonically grows in size during the execution, and it's freed once the query
execution is done. A general rule of thumb is to have double the RAM than what
the actual dataset is occupying.

## Configuration options to reduce memory usage

Here are several tips how you can reduce memory usage and increase scalability:

1. Consider removing label index by executing `DROP INDEX ON :Label;` 
2. Consider removing label-property index by executing `DROP INDEX
   ON :Label(property);` 
3. If you don't have properties on relationships, disable them in the
   configuration file by setting the `-storage-properties-on-edges` flag to
   `false`. This can significantly reduce memory usage because effectively
   `Edge` objects will not be created, and all information will be inlined under
   `Vertex` objects. You can disable properties on relationships with a
   non-empty database, if the relationships are without properties. If you need
   help with adapting the configuration to your needs, check out the the how-to
   guide on [changing configuration settings](/how-to-guides/config-logs.md). 

You can also check our reference guide for information about [controlling memory
usage](/reference-guide/memory-control.md), and you
[inspect](/reference-guide/optimizing-queries/inspecting-queries.md) and
[profile](/reference-guide/optimizing-queries/profiling-queries.md) your queries
to devise a plan for their optimization. 

## Control memory usage

In Memgraph, you can control memory usage by limiting, inspecting and
deallocating memory. 

You can control the memory usage of:
 - a whole instance by setting the `--memory-limit` within the configuration file 
 - a query by including the `QUERY MEMORY` clause at the end of a query
 - a procedure by including a `PROCEDURE MEMORY` clause

### Controlling the memory usage of an instance

By setting the `--memory-limit` flag in the configuration file, you can set
the maximum amount of memory (in MiB) that a Memgraph instance can allocate
during its runtime. If the memory limit is exceeded, only the queries that don't
require additional memory are allowed. If the memory limit is exceeded while a
query is running, the query is aborted and its transaction becomes invalid.

If the flag is set to 0, it will use the default values.
Default values are:
-  90% of the total memory if the system doesn't have swap memory.
-  100% of the total memory if the system has swap memory.

### Controlling the memory usage of a query

Each Cypher query can include the following clause at the end:

```plaintext
QUERY MEMORY ( UNLIMITED | LIMIT num (KB | MB) )
```

If you use the `LIMIT` option, you have to specify the amount of memory a query
can allocate for its execution. You can use this clause in a query only once at
the end of the query. The limit is applied to the entire query.

Examples: 
```plaintext
MATCH (n) RETURN (n) QUERY MEMORY LIMIT 10 KB;
```
```plaintext
MATCH (n) RETURN (n) QUERY MEMORY UNLIMITED;
```
### Controlling the memory usage of a procedure 

Each procedure call can contain the following clause:

```plaintext
PROCEDURE MEMORY ( UNLIMITED | LIMIT num ( KB | MB) )
```

If you use the `LIMIT` option, you can specify the amount of memory that the
called procedure can allocate for its execution. If you use the `UNLIMITED`
option, no memory restrictions will be imposed when the procedure is called. If
you don't specify the clause, the memory limit is set to a default value of 100 MB.

One procedure call can have only one `PROCEDURE MEMORY` clause at the end of the
call. If a query contains multiple procedure calls, each call can have its own
limit specification.

Examples:
```plaintext
CALL example.procedure(arg1, arg2, ...) PROCEDURE MEMORY LIMIT 100 KB YIELD result;
```
```plaintext
CALL example.procedure(arg1, arg2, ...) PROCEDURE MEMORY LIMIT 100 MB YIELD result;
```
```plaintext
CALL example.procedure(arg1, arg2, ...) PROCEDURE MEMORY UNLIMITED YIELD result;
```

### Inspecting memory usage

Run the following query to inspect memory usage:

```plaintext
SHOW STORAGE INFO;
```

Find out more about `SHOW STORAGE INFO` query on [Server stats](./server-stats.md).

### Deallocating memory

Memgraph has a garbage collector that deallocates unused objects, thus freeing
the memory. The rate of the garbage collection in seconds can be specified in
the configuration file by setting the `--storage-gc-cycle-sec`.

You can free up memory by using the following query:

```cypher
FREE MEMORY;
```

This query tries to clean up as much unused memory as possible without affecting
currently running transactions.

## Storage modes

Memgraph supports three different storage modes:
* `IN_MEMORY_TRANSACTIONAL` - the default database mode that favors
    strongly-consistent ACID transactions using WAL files and snapshots,
    but requires more time and resources during data import and analysis.
* `IN_MEMORY_ANALYTICAL` - speeds up import and data analysis but offers no ACID
    guarantees besides manually created snapshots.
* `ON_DISK_TRANSACTIONAL` - supports ACID properties in the same way as `IN_MEMORY_TRANSACTIONAL`
    with the additional ability to store data on disk (HDD or SSD) thus trading performance for lower costs. **Experimental**


### Switching storage modes

You can switch between in-memory modes within a session using the following query:

```cypher
STORAGE MODE IN_MEMORY_{TRANSACTIONAL|ANALYTICAL};
```

When switching modes, Memgraph will wait until all other transactions are done.
If some other transactions are running in your system, you will receive a
warning message, so be sure to [set the log level to
`WARNING`](/reference-guide/configuration.md#other).

Switching from the in-memory storage mode to the on-disk storage mode is allowed
when there is only one active session and the database is empty. As Memgraph Lab
uses multiple sessions to run queries in parallel, it is currently impossible to
switch to the on-disk storage mode within Memgraph Lab. You can change the
storage mode to on-disk transactional using `mgconsole`, then connect to the
instance with Memgraph Lab and query the instance as usual.

To change the storage mode to `ON_DISK_TRANSACTIONAL`, use the following query:

```cypher
STORAGE MODE ON_DISK_TRANSACTIONAL;
```

It is forbidden to change the storage mode from `ON_DISK_TRANSACTIONAL` to any
of the in-memory storage modes while there is data in the database as it might
not fit in the RAM. To change the storage mode to any of the in-memory storages,
empty the instance and restart it. An empty database will start in the default
storage mode (in-memory transactional).

If you are running the Memgraph Enterprise Edition, you need to have
[`STORAGE_MODE` permission](/reference-guide/auth-module.md) to change the
storage mode.

You can check the current storage mode using the following query:

```cypher
SHOW STORAGE INFO;
```

An empty instance will always restart in in-memory transactional storage mode.
Upon restart, a non-empty instance in the on-disk storage mode will not change
storage mode, but the instance in an in-memory analytical storage mode will revert
to the default in-memory transactional storage mode. 

### In-memory transactional storage mode (default)

`IN_MEMORY_TRANSACTIONAL` storage mode offers all ACID guarantees. WAL files and
periodic snapshots are created automatically, and you can also create snapshots
manually.

In the `IN_MEMORY_TRANSACTIONAL` mode, Memgraph creates a
[`Delta`](/memgraph/under-the-hood/storage#delta-memory-layout) object each time
data is changed. Deltas are the backbone upon which Memgraph provides atomicity,
consistency, isolation, and durability - ACID. By using `Deltas`, Memgraph
creates [write-ahead-logs](/memgraph/reference-guide/backup#write-ahead-logging)
for durability, provides isolation, consistency, and atomicity (by ensuring that
everything is executed or nothing).

Depending on the transaction [isolation
level](/memgraph/reference-guide/transactions#isolation-levels), other transactions may
see changes from other transactions.

In the transactional storage mode,
[snapshots](/memgraph/reference-guide/backup#snapshots) are created periodically
or manually. They capture the database state and store it on the disk. A
snapshot is used to recover the database upon startup (depending on the setting
of the configuration flag `--storage-recover-on-startup`, which defaults to
`true`).

When Memgraph starts creating a periodic snapshot, it is not possible to
manually create a snapshot, until the periodic snapshot is created.

Manual snapshots are created by running the `CREATE SNAPSHOT;` query.

### In-memory analytical storage mode

In the transactional storage mode, Memgraph is fully [ACID
compliant](/reference-guide/backup.md) which could cause memory spikes during data
import because each time data is changed Memgraph creates
[`Delta`](/memgraph/under-the-hood/storage#delta-memory-layout) objects to
provides atomicity, consistency, isolation, and durability

But `Deltas` also require a lot of memory (104B per change), especially when
there are a lot of changes  (for example, during import with the `LOAD CSV`
clause). By switching the storage mode to `IN_MEMORY_ANALYTICAL` mode disables
the creation of `Deltas` thus drastically speeding up import with lower memory
consumption - up to 6 times faster import with 6 times less memory consumption.

If you want to enable ACID compliance, you can switch back to
`IN_MEMORY_TRANSACTIONAL` and continue with regular work on the database or you
can take advantage of the low memory costs of the analytical mode to run
analytical queries that will not change the data, but be aware that no backup is
created automatically, and there are no ACID guarantees besides manually created
snapshots. There are no `WAL` files created nor periodic snapshots. Users
**can** create a snapshot manually.

#### Transactions

In the analytical storage mode, there are no ACID guarantees and other
transactions can see the changes of ongoing transactions. Also, a [transaction
can see the changes it is
doing](/memgraph/reference-guide/transactions#isolation-levels). This means that
the transactions can be committed in random orders, and the updates to the data,
in the end, might not be correct.

#### WAL

As mentioned, no [write-ahead
logs](/memgraph/reference-guide/backup#write-ahead-logging) are created in the
`IN_MEMORY_ANALYTICAL` mode. When switching back to the
`IN_MEMORY_TRANSACTIONAL` mode it is recommended to create a snapshot manually
with `CREATE SNAPSHOT;` Cypher query. Once Memgraph switches to the
`IN_MEMORY_TRANSACTIONAL` mode, for all new updates it will create a WAL if not
otherwise instructed by the config file.

#### Snapshots

[Snapshots](/memgraph/reference-guide/backup#snapshots) capture the database
state and store it on the disk. A snapshot is used to recover the database upon
startup (depending on the setting of the configuration flag
`--storage-recover-on-startup`, which defaults to `true`).

In Memgraph, snapshots are created periodically or manually.

In the `IN_MEMORY_ANALYTICAL` mode, periodic snapshots are **disabled**.

Manual snapshots are created by running the `CREATE SNAPSHOT;` query. When the
query is run in the `IN_MEMORY_ANALYTICAL` mode, Memgraph guarantees that it
will be **the only** transaction present in the system, and all the other
transactions will wait until the snapshot is created to ensure its validity.

### On-disk transactional storage mode

In the on-disk transactional storage mode, disk is used as a physical storage
which allows you to save more data than the capacity of your RAM. This helps
keep the hardware costs to a minimum, but you should except slower performance
when compared to the in-memory transactional storage mode. Keep in mind that
while executing queries, all the graph objects used in the transactions still
need to be able to fit in the RAM, or Memgraph will throw an exception. 

#### Architecture

RocksDB is used as a background storage to serialize nodes and relationships
into a key-value format. The used architecture is also known as "larger than
memory" as it enables in-memory databases to save more data than the main memory
can hold, without the performance overhead caused by the buffer pool. 

The imported data is residing on the disk, while the main memory contains two
caches, one executing operations on main RocksDB instance and the other for
operations that require indices. In both cases, Memgraph's custom
`SkipList` cache is used, which allows a multithreaded read-write access pattern.

#### MVCC

Concurrent execution of transactions is supported differently for on-disk
storage than for in-memory. The in-memory storage mode relies on delta objects
which store the exact versions of data at the specific moment in time.
Therefore, the in-memory storage mode uses a pessimistic approach and
immediately checks whether there is a conflict between two transactions. 

In the on-disk storage mode, the cache is used per transaction. This
significantly simplifies object management since there is no need to question
certain object's validity, but it also requires the optimistic approach for
conflict resolution between transactions. 

In the on-disk storage mode, the conflict is checked at the transaction's commit
time with the help of RocksDB's transaction support. This also implies that
deltas are cleared after each transaction, which can optimize memory usage
during execution. Deltas are still used to fully support Cypher's semantic of
the write queries. The design of the on-disk storage also simplifies the process
of garbage collection, since all the data is on disk.

#### Isolation level

The on-disk storage mode supports only snapshot isolation level. Mostly because
it's the Memgraph viewpoint that snapshot isolation should be the default
isolation level for most applications relying on databases. But the snapshot
isolation level also simplifies the query's execution flow since no data is transferred to the disk until the commit of the transaction.

#### Indices

The on-disk storage mode supports both label and label-property indices. They
are stored in separate RocksDB instances as key-value pairs so that the access
to the data is faster. Whenever the indexed node is accessed, it's stored into a
separate in-memory cache to maximize the reading speed.

#### Constraints

The on-disk storage mode supports both existence and uniqueness constraints.
Existence constraints don't use context from the disk since the validity of
nodes can be checked by looking only at this single node. On the other side,
uniqueness constraints require a different approach. For a node to be valid, the
engine needs to iterate through all other nodes under constraint and check
whether a conflict exists. To speed up this iteration process, nodes under
constraint are stored into a separate RocksDB instance to eliminate the cost of
iterating over nodes which are not under constraint.

#### Data formats

Below is the format in which data is serialized to the disk. 

Vertex format for main disk storage:
Key - `label1, label2, ... | vertex gid | commit_timestamp`
Value - `property1, property2`

Edge format for the main disk storage:
Key - `from vertex gid | to vertex gid | 0 | edge type | edge gid | commit_timestamp`
Value - `property1, property2`
`0` is a placeholder for edge direction in future.

Format for label index on disk:

Key - `indexing label | vertex gid | commit_timestamp`

Value - `label1_id, label2_id, ... | property1, property2, ...`

Value does not contain `indexing label`.

Format for label-property index on disk:

Key - `indexing label | indexing property | vertex gid | commit_timestamp`

Value - `label1_id, label2_id, ... | property1, property2, ...`

Value does not contain `indexing label`.

#### Durability

In the on-disk storage mode, durability is supported by RocksDB since it keeps
its own
[WAL](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-%28WAL%29) files.
Memgraph persists the metadata used in the implementation of the on-disk
storage. 

#### Memory control

If the workload is larger than memory, a single transaction must fit into the
memory. A memory tracker track all allocations happening throughout the
transaction's lifetime. Disk space also has to be carefully managed. Since the
timestamp is serialized together with the raw node and relationship data, the
engine needs to ensure that when the new version of the same node is stored, the
old one is deleted.

#### Replication

At the moment, the on-disk storage doesn't support replication.

